````markdown
# Credit DataOps (Airflow + Celery + Redis + Postgres + MinIO)

**ë¡œì»¬ ë°ì´í„° íŒŒì´í”„ë¼ì¸** í…œí”Œë¦¿ì…ë‹ˆë‹¤.  
Airflow(CeleryExecutor)ë¡œ **Bronze â†’ Silver â†’ Gold â†’ DW(Postgres)** íë¦„ì„ êµ¬ì¶•í•˜ê³ ,
MinIO(S3 í˜¸í™˜)ë¡œ íŒŒì¼ì„ ê´€ë¦¬í•©ë‹ˆë‹¤.

<br>

## ğŸ¯ ëª©ì 

- Airflow 3.xì˜ DAG/Task êµ¬ì„±ê³¼ **Celery** ê¸°ë°˜ ë¶„ì‚° ì‹¤í–‰
- **MinIO(S3)** ë¥¼ í™œìš©í•œ ì›ì²œ/ì •ì œ/ì§‘ê³„ ë°ì´í„° ê´€ë¦¬(Bronze/Silver/Gold)
- **Postgres** ë¡œ ê²°ê³¼ ì ì¬(ì—…ì„œíŠ¸), DBeaverë¡œ ì¡°íšŒ
- DAG ìŠ¤ì¼€ì¤„/íŠ¸ë¦¬ê±°/ë¡œê·¸/ì—ëŸ¬ í•¸ë“¤ë§ ë° **ìš´ì˜ íŒ**
- (ì˜µì…˜) Slack ì•Œë¦¼, Flower ëª¨ë‹ˆí„°ë§, Backfill

<br>

## ğŸ“¦ í”„ë¡œì íŠ¸ êµ¬ì¡°

```text
.
â”œâ”€â”€ docker-compose.yaml              # ë©”ì¸ ìŠ¤íƒ ì •ì˜ (Airflow/Redis/Postgres/MinIO ë“±)
â”œâ”€â”€ infra/
â”‚   â””â”€â”€ docker-compose.override.yml  # (ì„ íƒ) ì˜¤ë²„ë ˆì´
â”œâ”€â”€ dags/
â”‚   â”œâ”€â”€ hello.py                     # ê¸°ë³¸ í—¬ë¡œì›”ë“œ DAG
â”‚   â””â”€â”€ bronze_silver_gold_incremental.py  # í•µì‹¬ ì¦ë¶„ íŒŒì´í”„ë¼ì¸
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ airflow.cfg                  # Airflow ì„¤ì •(ë§ˆìš´íŠ¸)
â”‚   â””â”€â”€ config.yaml                  # (ì„ íƒ) ì‚¬ìš©ì ì •ì˜ ì„¤ì •
â”œâ”€â”€ logs/                            # Airflow ë¡œê·¸ ë³¼ë¥¨
â”œâ”€â”€ plugins/                         # ì»¤ìŠ¤í…€ ì˜¤í¼ë ˆì´í„°/í›…/ì„¼ì„œ ìœ„ì¹˜
â”œâ”€â”€ minio/                           # MinIO ì‹¤ì œ ì €ì¥ì†Œ ë³¼ë¥¨ (raw/silver/gold ìƒì„±ë¨)
â”‚   â”œâ”€â”€ raw
â”‚   â”œâ”€â”€ silver
â”‚   â””â”€â”€ gold
â””â”€â”€ requirements.txt                 # (ë¡œì»¬ ê°œë°œìš©) íŒŒì´ì¬ ì˜ì¡´ì„±
````

> ì°¸ê³ : `data/raw|silver|gold` ê°™ì€ **ë¡œì»¬ í´ë”ëŠ” í˜„ì¬ íŒŒì´í”„ë¼ì¸ì—ì„œ ì‚¬ìš©í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.**
> ì‹¤ì œ íŒŒì¼ì€ MinIO ë³¼ë¥¨(`./minio`) ì•„ë˜ì— ìƒê¹ë‹ˆë‹¤.

<br>

## ğŸš€ ì„¤ì¹˜ & ê¸°ë™

### (0) ì‚¬ì „ ì¤€ë¹„

* Docker / Docker Compose
* (ê¶Œì¥) Docker Desktop ì‚¬ìš© ì‹œ DNS/ë„¤íŠ¸ì›Œí¬ ì•ˆì •ì 

### (1) `.env` ì„¤ì •(í•„ìˆ˜ ì•„ë‹˜, composeì˜ `environment:`ì— ì§ì ‘ ë„£ì–´ë„ ë¨)

Airflow 3.0.4 + Python 3.12 **constraints** ë¥¼ ì§€ì •í•˜ê³ , í•„ìš”í•œ í”„ë¡œë°”ì´ë”ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.

```env
# Airflow ì´ë¯¸ì§€ íƒœê·¸
AIRFLOW_IMAGE_TAG=3.0.4

# Pip constraints (Py 3.12)
PIP_CONSTRAINT=https://raw.githubusercontent.com/apache/airflow/constraints-3.0.4/constraints-3.12.txt

# ë¶€ê°€ íŒ¨í‚¤ì§€(í…ŒìŠ¤íŠ¸ ìš©ë„) - ì»¨í…Œì´ë„ˆ ë¶€íŒ…ì‹œ ì„¤ì¹˜
# panderaëŠ” ë„¤íŠ¸ì›Œí¬/ë²„ì „ ì´ìŠˆê°€ ë¹ˆë²ˆí•˜ë‹ˆ 'ì˜µì…˜'ìœ¼ë¡œ ê¶Œì¥ (ì—†ì–´ë„ DAGê°€ ë™ì‘í•˜ë„ë¡ ì½”ë“œ ì²˜ë¦¬ë¨)
_PIP_ADDITIONAL_REQUIREMENTS=apache-airflow-providers-amazon apache-airflow-providers-postgres pandas apache-airflow-providers-slack
```

> âš ï¸ `_PIP_ADDITIONAL_REQUIREMENTS`ëŠ” **í…ŒìŠ¤íŠ¸ìš© ê¸°ëŠ¥**ì…ë‹ˆë‹¤.
> ìš´ì˜ì—ì„  ì»¤ìŠ¤í…€ ì´ë¯¸ì§€ë¥¼ ë¹Œë“œí•˜ì„¸ìš”.

### (2) Docker Compose ì£¼ìš” í¬ì¸íŠ¸

* `airflow-webserver`: `command: webserver` (CeleryExecutorì—ì„œ `standalone` ì§€ì–‘)
* `airflow-scheduler`: healthcheck í™œì„± (8974)
* `postgres`: í˜¸ìŠ¤íŠ¸ í¬íŠ¸ ì¶©ëŒ ì‹œ `5433:5432` ë§¤í•‘ ê¶Œì¥
* `minio`: 9000(S3 API), 9001(Console)

> (ì˜ˆì‹œëŠ” ê¸°ì¡´ ì €ì¥ì†Œì˜ `docker-compose.yaml`ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ë˜, ì´ë¯¸ì§€ íƒœê·¸/í—¬ìŠ¤ì²´í¬/í¬íŠ¸ë§Œ ì ê²€)

#### ì„œë¹„ìŠ¤ ëª©ë¡ í™•ì¸
```sh
docker compose config --services
```

### (3) ìŠ¤íƒ ê¸°ë™

```bash
docker compose up -d --build
docker compose ps
docker compose logs -f airflow-init   # ì´ˆê¸°í™” ì™„ë£Œ í™•ì¸
```

* Airflow UI: [http://localhost:8080](http://localhost:8080) (ê¸°ë³¸ `airflow` / `airflow`)
* MinIO ì½˜ì†”: [http://localhost:9001](http://localhost:9001) (ê¸°ë³¸ `minio` / `minio12345`)

### (4) Airflow Connections ìƒì„±

```bash
docker compose exec airflow-webserver bash -lc '
airflow connections add minio_s3 \
  --conn-type aws \
  --conn-login minio \
  --conn-password minio12345 \
  --conn-extra "{\"endpoint_url\":\"http://minio:9000\",\"region_name\":\"us-east-1\",\"aws_access_key_id\":\"minio\",\"aws_secret_access_key\":\"minio12345\"}"

airflow connections add dw_postgres \
  --conn-type postgres \
  --conn-host postgres \
  --conn-login airflow \
  --conn-password airflow \
  --conn-schema airflow \
  --conn-port 5432
airflow connections list
'
```

### AirFlow ë¹„ë°€ë²ˆí˜¸ 

#### ë¹„ë°€ë²ˆí˜¸ í™•ì¸
```
$ docker compose exec airflow-webserver bash -lc '
python - << "PY"
import os, json
p = os.environ.get("AIRFLOW_HOME","/opt/airflow") + "/simple_auth_manager_passwords.json.generated"
with open(p) as f:
    d = json.load(f)
print("username: airflow")
print("password:", d.get("airflow"))
PY
'
```

#### ë¹„ë°€ë²ˆí˜¸ ì¬ì„¤ì •
```
docker compose exec airflow-webserver bash -lc '
export NEWPW="1234";
python - << "PY"
import os, json
p = os.environ.get("AIRFLOW_HOME","/opt/airflow") + "/simple_auth_manager_passwords.json.generated"
with open(p) as f: d = json.load(f)
d["airflow"] = os.environ["NEWPW"]
with open(p, "w") as f: json.dump(d, f)
print("updated:", p)
PY
'
```

#### ì ìš©
```bash
docker compose restart airflow-webserver
```

> (ì˜µì…˜) Slack ì›¹í›…:

```bash
docker compose exec airflow-webserver airflow connections add slack_alert \
  --conn-type slackwebhook \
  --conn-password "https://hooks.slack.com/services/XXX/YYY/ZZZ"
```

<br>

## ë¹ ë¥¸ ìŠ¤ëª¨í¬ í…ŒìŠ¤íŠ¸: `hello_celery`

`dags/hello.py`

```python
from datetime import datetime
from airflow import DAG
from airflow.operators.bash import BashOperator

with DAG(
    dag_id="hello_celery",
    start_date=datetime(2024, 1, 1),
    schedule="@once",      # ì¼œë©´ í•œ ë²ˆ ì‹¤í–‰
    catchup=False,
    tags=["demo"],
) as dag:
    BashOperator(
        task_id="hello",
        bash_command="echo 'Hello from Airflow!'"
    )
```

ì‹¤í–‰:

```bash
docker compose exec airflow-webserver airflow dags list | grep hello_celery
docker compose exec airflow-webserver airflow dags unpause hello_celery
docker compose exec airflow-webserver airflow dags trigger  hello_celery
```

UI(Grid/Graph/Log)ì—ì„œ ì„±ê³µ(ì´ˆë¡) í™•ì¸.

<br>

## ğŸ§© í•µì‹¬ DAG: ì¦ë¶„ Bâ†’Sâ†’Gâ†’DW

`dags/bronze_silver_gold_incremental.py`

> **Airflow 3.x í•µì‹¬ ë³€ê²½ì‚¬í•­ ë°˜ì˜**
>
> * `schedule_interval` â†’ `schedule`
> * `get_current_context` ì„í¬íŠ¸ ê²½ë¡œ: `from airflow.operators.python import get_current_context`
> * ì»¨í…ìŠ¤íŠ¸ì—ì„œ ë‚ ì§œëŠ” `ds` ëŒ€ì‹  **`logical_date`** ì‚¬ìš© (ì˜ˆ: `strftime("%Y-%m-%d")`)

```python
from __future__ import annotations
from datetime import datetime, timedelta

from airflow import DAG
from airflow.decorators import task
from airflow.operators.python import get_current_context
import pandas as pd

# ì„ íƒ ì˜ì¡´ì„± ê°€ë“œ (pandera ì—†ì–´ë„ ë™ì‘)
try:
    import pandera as pa
    HAS_PANDERA = True
except Exception:
    HAS_PANDERA = False

# Slackë„ ì„ íƒ ì‚¬ìš©
try:
    from airflow.providers.slack.hooks.slack_webhook import SlackWebhookHook
    HAS_SLACK = True
except Exception:
    HAS_SLACK = False

S3_CONN_ID = "minio_s3"
PG_CONN_ID = "dw_postgres"
SLACK_CONN_ID = "slack_alert"  # ì—†ìœ¼ë©´ ë¬´ì‹œ

RAW_BUCKET = "raw"
SILVER_BUCKET = "silver"
GOLD_BUCKET = "gold"

if HAS_PANDERA:
    raw_schema = pa.DataFrameSchema({
        "customer_id": pa.Column(int),
        "state": pa.Column(str),
        "age": pa.Column(int),
        "churned": pa.Column(int),
    })

def slack_alert(context):
    if not HAS_SLACK:
        return
    try:
        dag_id = context["dag"].dag_id
        task_id = context["task_instance"].task_id
        run_id = context["run_id"]
        SlackWebhookHook(slack_webhook_conn_id=SLACK_CONN_ID).send(
            text=f":rotating_light: *Airflow Failed* â€” `{dag_id}.{task_id}` (run_id={run_id})"
        )
    except Exception as e:
        print("Slack alert error:", e)

default_args = {
    "owner": "demo",
    "retries": 1,
    "retry_delay": timedelta(minutes=5),
    "on_failure_callback": slack_alert,
}

with DAG(
    dag_id="bronze_silver_gold_incremental",
    start_date=datetime(2024, 1, 1),
    schedule="@daily",
    catchup=False,
    default_args=default_args,
    tags=["bsg", "incremental"],
) as dag:

    @task
    def seed_raw_if_empty() -> int:
        """ì˜¤ëŠ˜ì íŒŒí‹°ì…˜ì´ ë¹„ì–´ìˆìœ¼ë©´ ìƒ˜í”Œ csvë¥¼ rawì— ìƒì„±"""
        from airflow.providers.amazon.aws.hooks.s3 import S3Hook
        import io

        ds = get_current_context()["logical_date"].strftime("%Y-%m-%d")
        key = f"churn/dt={ds}/churn.csv"

        s3 = S3Hook(aws_conn_id=S3_CONN_ID)
        if not s3.check_for_key(key=key, bucket_name=RAW_BUCKET):
            df = pd.DataFrame({
                "customer_id": [1, 2, 3, 4, 5],
                "state": ["CA", "CA", "NY", "TX", "TX"],
                "age": [34, 45, 29, 41, 38],
                "churned": [0, 1, 0, 1, 0],
            })
            s3.load_string(df.to_csv(index=False), key=key, bucket_name=RAW_BUCKET, replace=True)
            return len(df)
        return 0

    @task
    def validate_raw() -> int:
        """ìŠ¤í‚¤ë§ˆ ê²€ì¦(pandera ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ ìµœì†Œ ê²€ì¦)"""
        from airflow.providers.amazon.aws.hooks.s3 import S3Hook
        import io

        ds = get_current_context()["logical_date"].strftime("%Y-%m-%d")
        key = f"churn/dt={ds}/churn.csv"

        s3 = S3Hook(aws_conn_id=S3_CONN_ID)
        body = s3.read_key(key=key, bucket_name=RAW_BUCKET)
        df = pd.read_csv(io.StringIO(body))

        if HAS_PANDERA:
            raw_schema.validate(df)
        else:
            required = {"customer_id","state","age","churned"}
            assert required <= set(df.columns), "columns mismatch"
            assert df.isna().sum().sum() == 0, "nulls found"
        return len(df)

    @task
    def to_silver() -> int:
        """ì •ì œ: state ëŒ€ë¬¸ì/ê³µë°± ì •ë¦¬"""
        from airflow.providers.amazon.aws.hooks.s3 import S3Hook
        import io

        ds = get_current_context()["logical_date"].strftime("%Y-%m-%d")
        raw_key = f"churn/dt={ds}/churn.csv"
        silver_key = f"churn/dt={ds}/churn_clean.csv"

        s3 = S3Hook(aws_conn_id=S3_CONN_ID)
        df = pd.read_csv(io.StringIO(s3.read_key(key=raw_key, bucket_name=RAW_BUCKET)))
        df["state"] = df["state"].str.upper().str.strip()

        s3.load_string(df.to_csv(index=False), key=silver_key, bucket_name=SILVER_BUCKET, replace=True)
        return len(df)

    @task
    def to_gold() -> int:
        """ì§‘ê³„: ì£¼(state)ë³„ ê³ ê°ìˆ˜/ì´íƒˆìˆ˜"""
        from airflow.providers.amazon.aws.hooks.s3 import S3Hook
        import io

        ds = get_current_context()["logical_date"].strftime("%Y-%m-%d")
        silver_key = f"churn/dt={ds}/churn_clean.csv"
        gold_key = f"churn/dt={ds}/churn_by_state.csv"

        s3 = S3Hook(aws_conn_id=S3_CONN_ID)
        df = pd.read_csv(io.StringIO(s3.read_key(key=silver_key, bucket_name=SILVER_BUCKET)))
        agg = df.groupby("state").agg(customers=("customer_id", "count"),
                                      churned=("churned", "sum")).reset_index()
        s3.load_string(agg.to_csv(index=False), key=gold_key, bucket_name=GOLD_BUCKET, replace=True)
        return len(agg)

    @task
    def load_gold_to_postgres() -> None:
        """(ds,state) PK ì—…ì„œíŠ¸"""
        from airflow.providers.amazon.aws.hooks.s3 import S3Hook
        from airflow.providers.postgres.hooks.postgres import PostgresHook
        import io

        ds = get_current_context()["logical_date"].strftime("%Y-%m-%d")
        gold_key = f"churn/dt={ds}/churn_by_state.csv"

        s3 = S3Hook(aws_conn_id=S3_CONN_ID)
        body = s3.read_key(key=gold_key, bucket_name=GOLD_BUCKET)
        agg = pd.read_csv(io.StringIO(body))
        agg.insert(0, "ds", ds)

        pg = PostgresHook(postgres_conn_id=PG_CONN_ID)
        pg.run("""
            CREATE TABLE IF NOT EXISTS churn_by_state_daily(
              ds DATE NOT NULL,
              state TEXT NOT NULL,
              customers INT,
              churned INT,
              PRIMARY KEY(ds, state)
            );
        """)
        with pg.get_conn() as conn, conn.cursor() as cur:
            for _, r in agg.iterrows():
                cur.execute("""
                    INSERT INTO churn_by_state_daily(ds, state, customers, churned)
                    VALUES (%s, %s, %s, %s)
                    ON CONFLICT (ds, state) DO UPDATE
                      SET customers=EXCLUDED.customers,
                          churned=EXCLUDED.churned;
                """, (r["ds"], r["state"], int(r["customers"]), int(r["churned"])))
            conn.commit()

    seed_raw_if_empty() >> validate_raw() >> to_silver() >> to_gold() >> load_gold_to_postgres()
```

<br>

## â–¶ï¸ ì‹¤í–‰/ê²€ì¦ ëª…ë ¹ ëª¨ìŒ

### DAG ë¡œë”© ì˜¤ë¥˜ í™•ì¸

```bash
docker compose exec airflow-webserver airflow dags list-import-errors
```

### DAG ì¸ì‹/ìƒíƒœ/ì‹¤í–‰

```bash
# DAG ì¸ì‹ í™•ì¸
docker compose exec airflow-webserver airflow dags list | grep bronze_silver_gold_incremental

# ì¼œê¸°(ì–¸íŒŒì¦ˆ) & ì¦‰ì‹œ ì‹¤í–‰
docker compose exec airflow-webserver airflow dags unpause bronze_silver_gold_incremental
docker compose exec airflow-webserver airflow dags trigger  bronze_silver_gold_incremental

# ì‹¤í–‰ ì´ë ¥ ë³´ê¸° (ë¹Œë“œì— ë”°ë¼ ë‘ í˜•ì‹ ì¤‘ í•˜ë‚˜ê°€ ë™ì‘)
docker compose exec airflow-webserver airflow dags list-runs bronze_silver_gold_incremental
# ë˜ëŠ”
docker compose exec airflow-webserver airflow dags list-runs --help
```

### ì§„í–‰ ë¡œê·¸

```bash
docker compose logs -f airflow-scheduler
docker compose logs -f airflow-worker
```

### MinIO ì‚°ì¶œë¬¼ í™•ì¸

```bash
docker compose exec mc sh -lc \
"mc alias set local http://minio:9000 minio minio12345 >/dev/null; \
 mc ls -r local/raw/churn; mc ls -r local/silver/churn; mc ls -r local/gold/churn"
```

### Postgres ê²°ê³¼ í™•ì¸

```bash
docker compose exec postgres bash -lc \
"psql -U airflow -d airflow -c \"SELECT * FROM churn_by_state_daily ORDER BY ds DESC, state;\""
```

<br>

## ğŸ§ª DBeaver ì ‘ì† (Windows)

`postgres` ì„œë¹„ìŠ¤ì— í¬íŠ¸ ë§¤í•‘:

```yaml
ports:
  - "127.0.0.1:5433:5432"   # ë¡œì»¬ 5433 ì‚¬ìš©(ì¶©ëŒ íšŒí”¼)
```

DBeaver ìƒˆ ì—°ê²°:

* Host: `127.0.0.1`
* Port: `5433`
* DB: `airflow`
* User/Pass: `airflow` / `airflow`

<br>

## ğŸ§° íŠ¸ëŸ¬ë¸”ìŠˆíŒ…(ìì£¼ ê²ªëŠ” ì´ìŠˆ)

* **DAG íŒŒì¼ì„ íŒŒì´ì¬ìœ¼ë¡œ ì§ì ‘ ì‹¤í–‰ ê¸ˆì§€**

  ```bash
  # ì˜ëª»ëœ ì˜ˆ
  python dags/my_dag.py  # X
  ```

  Airflowê°€ DAGì„ íŒŒì‹±/ì‹¤í–‰í•©ë‹ˆë‹¤.

* **ImportError / íŒŒì„œ ì—ëŸ¬**

  ```bash
  docker compose exec airflow-webserver airflow dags list-import-errors
  ```

  * Airflow 3.x: `schedule_interval` â†’ `schedule`
  * `get_current_context` ì„í¬íŠ¸ ê²½ë¡œ ì£¼ì˜
  * ì™¸ë¶€ íŒ¨í‚¤ì§€ ImportëŠ” `_PIP_ADDITIONAL_REQUIREMENTS` ë˜ëŠ” ì»¤ìŠ¤í…€ ì´ë¯¸ì§€
  * (ë³¸ ì˜ˆì œëŠ” `pandera` ì—†ì–´ë„ ë™ì‘í•˜ë„ë¡ ê°€ë“œ)

* **ì›Œì»¤ê°€ "ready"ë§Œ ì°íˆê³  ì¡°ìš©í•¨**

  * DAG **Paused ì—¬ë¶€** í™•ì¸ â†’ `airflow dags unpause <dag>`
  * ì‹¤í–‰ ì´ë ¥ í™•ì¸/íŠ¸ë¦¬ê±° â†’ `airflow dags list-runs â€¦`, `airflow dags trigger â€¦`

* **ìŠ¤ì¼€ì¤„ëŸ¬ unhealthy / ë²„ì „ ì¶©ëŒ**

  * `PIP_CONSTRAINT`(3.12) ì§€ì •, ì´ë¯¸ì§€ íƒœê·¸ **í†µì¼**
  * ì¶”ê°€ íŒ¨í‚¤ì§€ëŠ” constraints í•˜ì—ì„œ ì„¤ì¹˜

* **MinIO/PG ì—°ê²° ë¬¸ì œ**

  * Airflow Connections `minio_s3`, `dw_postgres` ì¬í™•ì¸
  * MinIO ì½˜ì†”ì—ì„œ ë²„í‚·/ê°ì²´ í™•ì¸

<br>

## ğŸ“ˆ í™•ì¥ ì•„ì´ë””ì–´

* Backfill:

  ```bash
  for d in 2025-08-15 2025-08-16 2025-08-17; do
    docker compose exec airflow-webserver airflow dags test bronze_silver_gold_incremental $d
  done
  ```
* Flower ëª¨ë‹ˆí„°ë§:

  ```bash
  docker compose --profile flower up -d flower   # http://localhost:5555
  ```
* ê²°ê³¼ Export:

  ```bash
  docker compose exec postgres bash -lc \
  "psql -U airflow -d airflow -c \"\\COPY churn_by_state_daily TO '/tmp/churn_by_state_daily.csv' CSV HEADER\" && \
   ls -l /tmp/churn_by_state_daily.csv"
  docker cp $(docker compose ps -q postgres):/tmp/churn_by_state_daily.csv ./data/churn_by_state_daily.csv
  ```
* í’ˆì§ˆ ê³ ë„í™”: Great Expectations/Pandera(ë„¤íŠ¸ì›Œí¬/í˜¸í™˜ í•´ê²° í›„ ì¬ë„ì…)
* ì•Œë¦¼: Slack/Email/Teams

<br>

## ìµœì¢… í™•ì¸ ì²´í¬ë¦¬ìŠ¤íŠ¸

* [ ] UIì—ì„œ `bronze_silver_gold_incremental` **ON**
* [ ] MinIO: `raw/silver/gold/churn/dt=YYYY-MM-DD/*.csv` ìƒì„±
* [ ] Postgres: `SELECT * FROM churn_by_state_daily;` ê²°ê³¼ í™•ì¸
* [ ] ì›Œì»¤/ìŠ¤ì¼€ì¤„ëŸ¬ ë¡œê·¸ì—ì„œ ì˜¤ë¥˜ ì—†ìŒ

---

